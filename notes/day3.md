用到的数学知识：
* 导数
* 方差
* 偏导数
* 对数
* 三角函数
* 矢量
* 海森矩阵


#### 迭代方法
1. 通过特征得到模型(预测函数)
2. 通过模型(预测函数)获得预测结果
3. 通过将预测结果与标签(实际结果)进行比较，计算其中的差距(损失)
4. 计算参数更新即更新模型(将预测函数中的数值向损失较小的部分进行调整)
5. 重复步骤2

#### 梯度下降法
- 可以在每一步上计算数据集的梯度，但是这样会产生大量的计算，效率极低，因此毫无必要。
- 计算小型数据样本的梯度效果很好。
    - 每一步抽取一个新的随机样本
* 随机梯度下降法
* 小批量梯度下降法

#### 学习速率
梯度下降法算法用梯度乘以一个称为学习速率（有时也称为步长）的标量，以确定下一个点的位置。

- 下一个点的位置 = 梯度 * 学习速率

超参数：是编程人员在机器学习算法中用于调整的旋钮

#### 优化学习速率
- 学习速率
- 步长

#### 随机梯度下降法
在梯度下降法中，**批量**指的是用于在单次迭代中计算梯度的样本总数。
到目前为止，我们一直假定批量是指整个数据集。就 Google 的规模而言，数据集通常包含数十亿甚至数千亿个样本。此外，Google 数据集通常包含海量特征。因此，一个批量可能相当巨大。如果是超大批量，则单次迭代就可能要花费很长时间进行计算。

包含随机抽样样本的大型数据集可能包含冗余数据。实际上，批量大小越大，出现冗余的可能性就越高。一些冗余可能有助于消除杂乱的梯度，但超大批量所具备的预测价值往往并不比大型批量高。

如果我们可以通过更少的计算量得出正确的平均梯度，会怎么样？通过从我们的数据集中随机选择样本，我们可以通过小得多的数据集估算（尽管过程非常杂乱）出较大的平均值。**随机梯度下降法 (SGD)**将这种想法运用到极致，它每次迭代只使用一个样本（批量大小为 1）。如果进行足够的迭代，SGD 也可以发挥作用，但过程会非常杂乱。“随机”这一术语表示构成各个批量的一个样本都是随机选择的。

**小批量随机梯度下降法（小批量 SGD）**是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。

[原文链接](https://developers.google.com/machine-learning/crash-course/reducing-loss/an-iterative-approach)
